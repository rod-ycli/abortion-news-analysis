{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGEXqM7iXSON"
   },
   "outputs": [],
   "source": [
    "# get_all_documents.py\n",
    "# Roderick Li and Jasmijn Cnossen\n",
    "# Language as Data\n",
    "# December 2021\n",
    "\n",
    "\"\"\"\n",
    "This script extracts all English and Dutch articles using MediaStack API and REST API.\n",
    "It removes empty texts & duplicates and saves the cleaned datasets as tsv files.\n",
    "\n",
    "First, we will define some functions.\n",
    "After that, the code starts.\n",
    "It will take approximately 10 minutes to run this script\n",
    "\"\"\"\n",
    "\n",
    "# import all packages\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import html5lib\n",
    "import http.client, urllib.parse, json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# We keep track of the time it takes to run the code:\n",
    "timer = time\n",
    "start = timer.perf_counter()\n",
    "\n",
    "########################\n",
    "### DEFINE FUNCTIONS ###\n",
    "########################\n",
    "\n",
    "def url_to_string(url):\n",
    "    \"\"\"\n",
    "    Extracts the raw text from a web page.\n",
    "    It takes a URL string as input and returns the text.\n",
    "    \"\"\"\n",
    "    \n",
    "    parser_content = url_to_html(url)\n",
    "    return html_to_string(parser_content)\n",
    "    \n",
    "def html_to_string(parser_content):\n",
    "    \"\"\"Extracts the textual content from an html object.\"\"\"\n",
    "    \n",
    "    # Remove scripts\n",
    "    for script in parser_content([\"script\", \"style\", \"aside\"]):\n",
    "        script.extract()\n",
    "        \n",
    "    # This is a shorter way to write the code for removing the newlines.\n",
    "    # It does it in one step without intermediate variables\n",
    "    return \" \".join(re.split(r'[\\n\\t]+', parser_content.get_text()))\n",
    "    \n",
    "def url_to_html(url):\n",
    "    \"\"\"Scrapes the html content from a web page. Takes a URL string as input and returns an html object. \"\"\"\n",
    "    \n",
    "    # Get the html content\n",
    "    res = requests.get(url, headers={\"User-Agent\": \"XY\"})\n",
    "#     res = requests.get(url + \".pdf\", headers={\"User-Agent\": \"XY\"})\n",
    "    html = res.text\n",
    "    parser_content = BeautifulSoup(html, 'html5lib')\n",
    "    return parser_content\n",
    "\n",
    "# We are looking for the author information at places where it can often be found.\n",
    "# If we do not find it, it does not mean that it is not there.\n",
    "def parse_author(html_content):\n",
    "    \n",
    "    # Initialize variables\n",
    "    search_query = re.compile('author', re.IGNORECASE)\n",
    "    name = \"\"\n",
    "    \n",
    "    # The author information might be encoded as a value of the attribute name\n",
    "    attribute = html_content.find('meta', attrs={'name': search_query})\n",
    "    \n",
    "    # Or as a property\n",
    "    property = html_content.find('meta', property=search_query)\n",
    "\n",
    "    found_author = attribute or property\n",
    "    \n",
    "    if found_author:\n",
    "        name = found_author['content']\n",
    "   \n",
    "   # If the author name cannot be found in the metadata, we might find it as an attribute of the text.\n",
    "    else:\n",
    "        itemprop = html_content.find(attrs={'itemprop': 'author'})\n",
    "        byline = html_content.find(attrs={'class': 'byline'})\n",
    "    \n",
    "        found_author = itemprop or byline\n",
    "        \n",
    "        if found_author:\n",
    "            name = found_author.text\n",
    "    \n",
    "    name = name.replace(\"by \", \"\")\n",
    "    name = name.replace(\"\\n\", \"\")\n",
    "    return name.strip()\n",
    "\n",
    "#This function requires the HTML content of the result as an input parameter\n",
    "#It returns the actual text content\n",
    "def parse_news_text(html_content):\n",
    "\n",
    "    # Try to find Article Body by Semantic Tag\n",
    "    article = html_content.find('article')\n",
    "\n",
    "    # Otherwise, try to find Article Body by Class Name (with the largest number of paragraphs)\n",
    "    if not article:\n",
    "        articles = html_content.find_all(class_=re.compile('(body|article|main)', re.IGNORECASE))\n",
    "        if articles:\n",
    "            article = sorted(articles, key=lambda x: len(x.find_all('p')), reverse=True)[0]\n",
    "\n",
    "    # Parse text from all Paragraphs\n",
    "    text = []\n",
    "    if article:\n",
    "        paragraphes = [tag.text for tag in article.find_all('p')]\n",
    "        for paragraph in paragraphes:\n",
    "                if re.findall(\"[.,!?]\", paragraph):\n",
    "                    text.append(paragraph)\n",
    "    text = re.sub(r\"\\s+\", \" \", \" \".join(text))\n",
    "\n",
    "    return text\n",
    "\n",
    "# Function to extract metadata from an article\n",
    "def extract_metadata(article):\n",
    "    # Extract the publication date\n",
    "    published_at = article['published_at']\n",
    "    if published_at:\n",
    "        date, time = published_at.split(\"T\")        \n",
    "    else:\n",
    "        date = \"\"\n",
    "        time = \"\"\n",
    "\n",
    "    # Extract meta data\n",
    "    url = article['url']\n",
    "    title= article['title'] \n",
    "    \n",
    "    category = article['category'] # category associated with the given news article\n",
    "    country = article['country'] # country code associated with given article \n",
    "    source = article['source'] # news source\n",
    "\n",
    "    return date, time, title, url, category, country, source\n",
    "\n",
    "# Function for the search query\n",
    "def search_articles(language, keywords, date):\n",
    "    \"\"\"\n",
    "    language: 'en' or 'nl'\n",
    "    keywords: 'abortion' or 'abortus'\n",
    "    date: e.g. '2021-01-01,2021-11-16'\n",
    "    \"\"\"\n",
    "    conn = http.client.HTTPConnection('api.mediastack.com')\n",
    "\n",
    "    params = urllib.parse.urlencode({\n",
    "        'access_key': ' ',  ## enter access key\n",
    "        'keywords': keywords,\n",
    "        'sort': 'published_desc',\n",
    "        'languages': language,\n",
    "        'limit': 100,\n",
    "        'date': date\n",
    "        })\n",
    "\n",
    "    conn.request('GET', '/v1/news?{}'.format(params))\n",
    "    res = conn.getresponse()\n",
    "    data = res.read()\n",
    "    query_content=(data.decode('utf-8'))\n",
    "    \n",
    "    return json.loads(query_content)\n",
    "\n",
    "# Save search query results as tsv\n",
    "def save_query_as_tsv(queries, outfile):\n",
    "    \"\"\"\n",
    "    queries: list of one or multiple queries for same language\n",
    "    oufile: tsv file to save the results in\n",
    "    \"\"\"\n",
    "\n",
    "    with open(outfile, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(\"Publication Date\\tTime\\tAuthor\\tSource\\tTitle\\tURL\\tText\\n\")\n",
    "      \n",
    "        for i, query in enumerate(queries):\n",
    "            articles = query[\"data\"]\n",
    "\n",
    "            for i, article in enumerate(articles):\n",
    "                # Extract metadata\n",
    "                date, time, title, article_url, category, country, source = extract_metadata(article)\n",
    "                \n",
    "                # Extract content\n",
    "                article_content = url_to_html(article_url)\n",
    "                author = parse_author(article_content)\n",
    "                try:\n",
    "                    content = parse_news_text(article_content) # try to extract the text\n",
    "                except:\n",
    "                    content = \"\" # no text if text cannot be extracted\n",
    "                \n",
    "                # Remove the newlines and tabulars from the content, title and author to avoid problems when saving as tsv file\n",
    "                content = content.replace(\"\\n\", \"\")\n",
    "                content = content.replace(\"\\t\", \"\")\n",
    "                title = title.replace(\"\\n\", \"\")\n",
    "                title = title.replace(\"\\t\", \"\")\n",
    "                author = author.replace(\"\\t\", \"\")\n",
    "                author = author.replace(\"By \", \"\")\n",
    "                \n",
    "                # Separate fields by tabulators (\\t)\n",
    "                output = \"\\t\".join([date, time, author, source, title, article_url, content])\n",
    "                f.write(output +\"\\n\")\n",
    "\n",
    "# Put info from query in a dataframe\n",
    "def query_to_dataframe(queries):\n",
    "    \"\"\"\n",
    "    queries: list of one or multiple queries for same language\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=[\"Publication Date\",\"Time\",\"Author\",\"Source\",\"Title\",\"URL\",\"Text\"])\n",
    "      \n",
    "    for i, query in enumerate(queries):\n",
    "        articles = query[\"data\"]\n",
    "\n",
    "        for i, article in enumerate(articles):\n",
    "            # Extract metadata\n",
    "            date, time, title, article_url, category, country, source = extract_metadata(article)\n",
    "            \n",
    "            # Extract content\n",
    "            article_content = url_to_html(article_url)\n",
    "            author = parse_author(article_content)\n",
    "            try:\n",
    "                content = parse_news_text(article_content) # try to extract the text\n",
    "            except:\n",
    "                content = \"\" # no text if text cannot be extracted\n",
    "            \n",
    "            # Remove the newlines and tabulars from the content, title and author to avoid problems when saving as tsv file\n",
    "            content = content.replace(\"\\n\", \"\")\n",
    "            content = content.replace(\"\\t\", \"\")\n",
    "            title = title.replace(\"\\n\", \"\")\n",
    "            title = title.replace(\"\\t\", \"\")\n",
    "            author = author.replace(\"\\t\", \"\")\n",
    "            author = author.replace(\"By \", \"\")\n",
    "            \n",
    "            # Append to dataframe\n",
    "            output = pd.DataFrame([[date, time, author, source, title, article_url, content]], columns=[\"Publication Date\",\"Time\",\"Author\",\"Source\",\"Title\",\"URL\",\"Text\"])\n",
    "            df = df.append(output, ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function that removes duplicate and empty entries in a column of a dataframe\n",
    "def remove_duplicates_and_empty(dataframe, column_name):\n",
    "    \"\"\"\n",
    "    dataframe: e.g. content_eng or content_nld\n",
    "    column_name: name of column for which to remove duplicates and empty entries, e.g. \"Text\"\n",
    "    returns dataframe without rows containing duplicate or empty entries in column_name\n",
    "    \"\"\"\n",
    "    unique_content = []\n",
    "    for i, content in enumerate(dataframe[column_name]):\n",
    "        # check whether we have already encountered this content before\n",
    "        if content not in unique_content:\n",
    "            unique_content.append(content)\n",
    "        else:\n",
    "            dataframe[column_name][i] = \"\" # replace the duplicate entry with empty string\n",
    "\n",
    "    # only keep rows where column_name is not empty string\n",
    "    rows_to_drop = dataframe.loc[dataframe[column_name] == \"\"]\n",
    "    dataframe = dataframe.drop(rows_to_drop.index)\n",
    "\n",
    "    return dataframe.reset_index(drop=True)\n",
    "\n",
    "# Function that removes rows from dataframe where column_name is too short\n",
    "def remove_short(dataframe, column_name, min_char):\n",
    "    \"\"\"\n",
    "    dataframe: e.g. content_eng or content_nld\n",
    "    column_name: name of column for which to remove short entries, e.g. \"Text\"\n",
    "    min: the minimum number of characters an entry should have\n",
    "    \n",
    "    returns dataframe without rows for which entry in column_name is smaller than 'min' characters\n",
    "    \"\"\"\n",
    "    for i, content in enumerate(dataframe[column_name]):\n",
    "        # if length is shorter than 'min', remove content\n",
    "        if len(content) < min_char:\n",
    "            dataframe[column_name][i] = \"\"\n",
    "\n",
    "    # only keep rows where column_name is not empty string\n",
    "    rows_to_drop = dataframe.loc[dataframe[column_name] == \"\"]\n",
    "    dataframe = dataframe.drop(rows_to_drop.index)\n",
    "    \n",
    "    return dataframe.reset_index(drop=True)\n",
    "\n",
    "# Function to remove junk sentences from Dutch texts\n",
    "def remove_junk_nld(dataframe):\n",
    "    \"\"\"\n",
    "    dataframe: content_nld_clean / content_nld\n",
    "    returns dataframe where specific junk sentences from the Dutch articles are removed\n",
    "    \"\"\"\n",
    "    junk_text = [\"GFC NIEUWSREDACTIE –\",\n",
    "                 \"GFC NIEUWSREDACTIE-\", \n",
    "                 \"Je gebruikt een adblocker. Wij kunnen onze artikelen alleen gratis toegankelijk voor je maken dankzij advertenties. Wil je jouw adblocker voor ons pauzeren?\", \n",
    "                 \"Zoek dan via de zogenaamde ISIN code. Elk instrument, aandeel etc. heeft een unieke code. Kies vervolgens - wanneer er meerdere resultaten zijn - de notering op de beurs van uw keuze. Google de naam van het instrument, aandeel etc. met de toevoeging 'ISIN'. Als zoeken op ISIN code geen resultaten oplevert hebben wij het instrument of aandeel niet in onze koersendatabase.\",\n",
    "                 \"1. wanneer u op de PDF-button rechtsonder klikt, krijgt u het printvenster 2. u maakt dan de afdrukkeuze PDF 3. u kiest vervolgens save 4. het wordt nu opgeslagen als PDF 5. Wij wensen u veel leesplezier\",\n",
    "                 \"N.B. Het kan zijn dat elementen ontbreken aan deze printversie.\"]\n",
    "\n",
    "    for i, content in enumerate(dataframe[\"Text\"]):\n",
    "        # for this text, remove article completely:\n",
    "        if \"Heeft u al een account? \" in content:\n",
    "            dataframe[\"Text\"][i] = \"\"\n",
    "        # for junk texts, remove text and leave rest of article in:\n",
    "        for text in junk_text:\n",
    "            if text in content:\n",
    "                new_content = content.replace(text, \"\")\n",
    "                dataframe[\"Text\"][i] = new_content\n",
    "    \n",
    "    # only keep rows where column_name is not empty string\n",
    "    rows_to_drop = dataframe.loc[dataframe[\"Text\"] == \"\"]\n",
    "    dataframe = dataframe.drop(rows_to_drop.index)\n",
    "\n",
    "    return dataframe.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPGnOabJcEjw"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "### SCRAPE THE DOCUMENTS ###\n",
    "############################\n",
    "\n",
    "### ENGLISH ###\n",
    "# We do 3 search queries:\n",
    "query_en1 = search_articles('en', 'abortion', '2021-11-01,2021-11-10') # November 2021\n",
    "query_en2 = search_articles('en', 'abortion', '2021-10-01,2021-10-31') # October 2021\n",
    "query_en3 = search_articles('en', 'abortion', '2021-09-01,2021-09-30') # September 2021\n",
    "\n",
    "### DUTCH ###\n",
    "# For Dutch, we could only do 1 query because MediaStack didn't have more than 94 articles from this year\n",
    "query_nl = search_articles('nl', 'abortus', '2021-01-01,2021-11-16')\n",
    "\n",
    "# We save the results from the queries in dataframes\n",
    "# The function query_to_dataframe extracts metadata from the query and cleans the text from tabulars (\\t) and new lines (\\n)\n",
    "content_eng = query_to_dataframe([query_en1, query_en2, query_en3])\n",
    "content_nld = query_to_dataframe([query_nl])\n",
    "\n",
    "# We scrape the rest of the Dutch articles with REST API and add it to content_nld\n",
    "\n",
    "# De Volkskrant\n",
    "search_request = \"https://www.volkskrant.nl/search?query=abortus\"\n",
    "search_results = url_to_html(search_request)\n",
    "urls = search_results.select(\"a.teaser__link\")\n",
    "outputs = []\n",
    "count = 0\n",
    "\n",
    "for url in urls:        \n",
    "    # Getting each url\n",
    "    article_url = \"https://www.volkskrant.nl/\"+url[\"href\"]\n",
    "\n",
    "    # Getting each article\n",
    "    article_content = url_to_html(article_url)\n",
    "\n",
    "    # Extract metadata\n",
    "    date = article_content.select(\"span.artstyle__byline__date\")[0].string\n",
    "    time = article_content.select(\"span.artstyle__byline__time\")[0].string\n",
    "    date = date.strip()\n",
    "    day, month, year = date.split()\n",
    "    day = int(day)\n",
    "    if month == \"oktober\":\n",
    "        month = 10\n",
    "    if month == \"november\":\n",
    "        month = 11\n",
    "    if month == \"december\":\n",
    "        month = 12\n",
    "\n",
    "    if (day>10 and month==11) or (month==12): # Skip if the article is later than 10 Nov\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        if len(str(day))==1:\n",
    "            day = f\"0{day}\"\n",
    "        date = f\"{year}-{month}-{day}\"\n",
    "        title = article_content.select(\"h1.artstyle__header-title\")[0].string.replace(\"\\n\", \"\")\n",
    "        source = \"De Volkskrant\"\n",
    "\n",
    "        author = parse_author(article_content)\n",
    "\n",
    "        # Extract content\n",
    "        content = parse_news_text(article_content)\n",
    "        content = content.replace(\"\\n\", \"\")\n",
    "\n",
    "        output = pd.DataFrame([[date, time, author, source, title, article_url, content]], columns=[\"Publication Date\",\"Time\",\"Author\",\"Source\",\"Title\",\"URL\",\"Text\"])\n",
    "        content_nld = content_nld.append(output, ignore_index=True)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    if count == 12:\n",
    "        break\n",
    "\n",
    "# Metro nieuws\n",
    "search_request = \"https://www.metronieuws.nl/?s=abortus\"\n",
    "search_results = url_to_html(search_request)\n",
    "urls = search_results.select(\"a.list__link\")\n",
    "outputs = []\n",
    "count = 0\n",
    "\n",
    "for url in urls:\n",
    "    # Getting each url\n",
    "    article_url = url[\"href\"]\n",
    "\n",
    "    # Getting each article\n",
    "    article_content = url_to_html(article_url)\n",
    "\n",
    "    # Extract metadata\n",
    "    datetime = article_content.select(\"span.meta__date\")[0].string\n",
    "    date, time = datetime.split(\"/\")\n",
    "    date = date.strip()\n",
    "    day, month, year = date.split()\n",
    "    day = int(day)\n",
    "    if month == \"aug\":\n",
    "        month = \"08\"\n",
    "    if month == \"sep\":\n",
    "        month = \"09\"\n",
    "    if month == \"okt\":\n",
    "        month = 10\n",
    "    if month == \"nov\":\n",
    "        month = 11\n",
    "    if month == \"dec\":\n",
    "        month = 12\n",
    "\n",
    "    if (day>10 and month==11) or (month==12): # Skip if the article is later than 10 Nov\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        if len(str(day))==1:\n",
    "            day = f\"0{day}\"\n",
    "        date = f\"{year}-{month}-{day}\"\n",
    "\n",
    "        title = article_content.select(\"h1.article__title\")[0].string\n",
    "        source = \"metronieuws.nl\"\n",
    "\n",
    "        author = parse_author(article_content)\n",
    "\n",
    "        # Extract content\n",
    "        content = parse_news_text(article_content)\n",
    "        content = content.replace(\"\\n\", \"\")\n",
    "\n",
    "        output = pd.DataFrame([[date, time, author, source, title, article_url, content]], columns=[\"Publication Date\",\"Time\",\"Author\",\"Source\",\"Title\",\"URL\",\"Text\"])\n",
    "        content_nld = content_nld.append(output, ignore_index=True)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    if count == 12:\n",
    "        break\n",
    "\n",
    "# Het Parool\n",
    "search_request = \"https://www.parool.nl/search?query=abortus\"\n",
    "search_results = url_to_html(search_request)\n",
    "urls = search_results.select(\"a.teaser__link\")\n",
    "outputs = []\n",
    "count = 0\n",
    "\n",
    "for url in urls:\n",
    "    # Getting each url\n",
    "    article_url = \"https://www.parool.nl\"+url[\"href\"]\n",
    "\n",
    "    # Getting each article\n",
    "    article_content = url_to_html(article_url)\n",
    "\n",
    "    # Extract metadata\n",
    "    time = article_content.select(\"span.artstyle__byline__time\")[0].string\n",
    "\n",
    "    date = article_content.select(\"span.artstyle__byline__date\")[0].string\n",
    "    day, month, year = date.split()\n",
    "    day = int(day)\n",
    "    dutch_month = [\"januari\", \"februari\", \"maart\", \"april\", \"mei\", \"juni\", \"juli\", \"augustus\", \"september\", \"oktober\", \"november\", \"december\"]\n",
    "    for mon in dutch_month:\n",
    "        if mon==month:\n",
    "            month = dutch_month.index(mon)+1\n",
    "\n",
    "    if (day>10 and month==11) or (month==12): # Skip if the article is later than 10 Nov\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        if len(str(day))==1:\n",
    "            day = f\"0{day}\"\n",
    "        if len(str(month))==1:\n",
    "            month = f\"0{month}\"\n",
    "        date = f\"{year}-{month}-{day}\"\n",
    "\n",
    "        title = article_content.select(\"h1.artstyle__header-title \")[0].string.replace(\"\\n\",\"\")\n",
    "        source = \"Het Parool\"\n",
    "\n",
    "        author = parse_author(article_content)\n",
    "\n",
    "        # Extract content\n",
    "        content = parse_news_text(article_content)\n",
    "        content = content.replace(\"\\n\", \"\")\n",
    "\n",
    "        output = pd.DataFrame([[date, time, author, source, title, article_url, content]], columns=[\"Publication Date\",\"Time\",\"Author\",\"Source\",\"Title\",\"URL\",\"Text\"])\n",
    "        content_nld = content_nld.append(output, ignore_index=True)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    if count == 19:\n",
    "        break\n",
    "\n",
    "# Trouw\n",
    "search_request = \"https://www.trouw.nl/search?query=abortus\"\n",
    "search_results = url_to_html(search_request)\n",
    "urls = search_results.select(\"a.teaser__link\")\n",
    "outputs = []\n",
    "count = 0\n",
    "\n",
    "for url in urls:\n",
    "    # Getting each url\n",
    "    article_url = \"https://www.trouw.nl\"+url[\"href\"]\n",
    "\n",
    "    # Getting each article\n",
    "    article_content = url_to_html(article_url)\n",
    "\n",
    "    # Extract metadata\n",
    "    time = article_content.select(\"span.artstyle__byline__time\")[0].string\n",
    "\n",
    "    date = article_content.select(\"span.artstyle__byline__date\")[0].string\n",
    "    day, month, year = date.split()\n",
    "    day = int(day)\n",
    "    dutch_month = [\"januari\", \"februari\", \"maart\", \"april\", \"mei\", \"juni\", \"juli\", \"augustus\", \"september\", \"oktober\", \"november\", \"december\"]\n",
    "    for mon in dutch_month:\n",
    "        if mon==month:\n",
    "            month = dutch_month.index(mon)+1\n",
    "\n",
    "    if (day>10 and month==11) or (month==12): # Skip if the article is later than 10 Nov\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        if len(str(day))==1:\n",
    "            day = f\"0{day}\"\n",
    "        if len(str(month))==1:\n",
    "            month = f\"0{month}\"\n",
    "        date = f\"{year}-{month}-{day}\"\n",
    "\n",
    "        title = article_content.select(\"h1.artstyle__header-title \")[0].string.replace(\"\\n\",\"\")\n",
    "        source = \"Trouw\"\n",
    "\n",
    "        author = parse_author(article_content)\n",
    "\n",
    "        # Extract content\n",
    "        content = parse_news_text(article_content)\n",
    "        content = content.replace(\"\\n\", \"\")\n",
    "        content = content.replace(' Om u deze content te kunnen laten zien, hebben wij uw toestemming nodig om cookies te plaatsen. Open uw cookie-instellingen om te kiezen welke cookies u wilt accepteren. Voor een optimale gebruikservaring van onze site selecteert u \"Accepteer alles\". U kunt ook alleen de sociale content aanzetten: vink hiervoor \"Cookies accepteren van sociale media\" aan.', \"\")\n",
    "\n",
    "        output = pd.DataFrame([[date, time, author, source, title, article_url, content]], columns=[\"Publication Date\",\"Time\",\"Author\",\"Source\",\"Title\",\"URL\",\"Text\"])\n",
    "        content_nld = content_nld.append(output, ignore_index=True)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    if count == 20:\n",
    "        break\n",
    "\n",
    "\n",
    "#################################\n",
    "### CLEAN DATAFRAMES AND TEXT ###\n",
    "#################################\n",
    "\n",
    "# Replace missing values\n",
    "content_eng[\"Text\"] = content_eng[\"Text\"].fillna(\"\")\n",
    "content_eng[\"Author\"] = content_eng[\"Author\"].fillna(\"Unknown\")\n",
    "\n",
    "content_nld[\"Text\"] = content_nld[\"Text\"].fillna(\"\")\n",
    "content_nld[\"Author\"] = content_nld[\"Author\"].fillna(\"Unknown\")\n",
    "\n",
    "# Remove rows for which \"Text\" and/or \"Title\" is a duplicate or empty\n",
    "content_eng_clean = remove_duplicates_and_empty(content_eng, \"Text\")\n",
    "content_eng_clean = remove_duplicates_and_empty(content_eng_clean, \"Title\")\n",
    "\n",
    "content_nld_clean = remove_duplicates_and_empty(content_nld, \"Text\")\n",
    "content_nld_clean = remove_duplicates_and_empty(content_nld_clean, \"Title\")\n",
    "\n",
    "# Remove junk sentences from Dutch articles\n",
    "content_nld_clean = remove_junk_nld(content_nld_clean)\n",
    "\n",
    "# Remove entries where Text is shorter than 500 characters. Here, extraction probably went wrong and the text is either a junk message (e.g. 'page not found' messages) or just too short to be useful.\n",
    "content_eng_clean = remove_short(content_eng_clean, \"Text\", 500)\n",
    "content_nld_clean = remove_short(content_nld_clean, \"Text\", 500)\n",
    "\n",
    "print(\"nr_articles eng:\", len(content_eng_clean))\n",
    "print(\"nr_articles nld:\", len(content_nld_clean))\n",
    "\n",
    "\n",
    "# Save cleaned dataset as tsv\n",
    "# English\n",
    "tsv_file = \"data/eng/abortion_overview_clean2.tsv\"\n",
    "content_eng_clean.to_csv(tsv_file, sep=\"\\t\", index=False)\n",
    "\n",
    "# Dutch\n",
    "tsv_file = \"data/nld/abortus_overview_clean2.tsv\"\n",
    "content_nld_clean.to_csv(tsv_file, sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "### CALCULATE THE TIME IT TOOK TO RUN ALL CODE ###\n",
    "stop = timer.perf_counter()\n",
    "the_time = stop - start\n",
    "minutes = int(the_time / 60)\n",
    "print(f\"Time it took to run the code: {minutes} minutes\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "get_all_documents.py example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
