{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_all_analyses.py example.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# run_all_analyses.py\n","\n","Here, we can combine our code into one code file. This code is not meant to be run here in this notebook, just to combine the code. When it is done we can copy paste the code to a .py file and run it to check whether everything works.\n","\n","I put the functions in the first block and the rest of the code in the second block.\n","\n","This code assumes that the folder \"data\" contains: \n","* eng/abortion_overview_clean.tsv\n","* nld/abortus_overview_clean.tsv\n","* mrc2.dct.txt\n","* concreteness.xlsx\n","* AoA.xlsx\n","* wiki-news-300d-1M.vec\n","* cc.nl.300.vec"],"metadata":{"id":"z_RqH7fVzA4J"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8wCM2Z6uxvhV"},"outputs":[],"source":["# run_all_analyses.py\n","# Roderick Li and Jasmijn Cnossen\n","# Language as Data\n","# December 2021\n","\n","\"\"\"\n","This script will run several semantic and stylistic analyses on our English and Dutch datasets.\n","\n","First, we will define some functions to make the code more clear.\n","You can scroll down to were our analyses start.\n","It will take approximately ... minutes to run this script\n","\"\"\"\n","\n","# import all packages\n","import time\n","import pandas as pd\n","import numpy as np\n","import stanza\n","import string\n","import statistics\n","from collections import Counter, OrderedDict\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","from nltk.corpus import stopwords\n","import nltk\n","nltk.download('alpino')\n","from nltk.corpus import alpino\n","from sklearn.cluster import KMeans\n","import random\n","from gensim.models import KeyedVectors\n","from sklearn.manifold import TSNE\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","# random.seed(10)\n","\n","# Load tokenizer from NLTK\n","nltk.download('punkt')\n","from nltk import word_tokenize, sent_tokenize\n","# If the code above bugs out, install additional dependencies, or use the following code:\n","# from nltk.tokenize import ToktokTokenizer (naive tokenizer)\n","# import re\n","# sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n","# toktok = ToktokTokenizer()\n","# word_tokenize = toktok.tokenize\n","\n","# pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n","\n","# We keep track of the time it takes to run the code:\n","start = time.perf_counter()\n","\n","########################\n","### DEFINE FUNCTIONS ###\n","########################\n","\n","### PREPROCESSING ###\n","\n","# functions for reading in the data, tokenizing it with stanza, apply preprocessing, remove infrequent words\n","\n","def tokenize_articles(dataframe, iso1):\n","    \"\"\"\n","    dataframe: pandas dataframe with articles\n","    iso1: \"en\" or \"nl\"\n","\n","    returns list of documents tokenized with stanza\n","    \"\"\"\n","    articles = dataframe[\"Text\"]\n","    \n","    # load stanza tokenizer\n","    stanza.download(iso1)\n","    tokenizer = stanza.Pipeline(iso1, processors='tokenize')\n","    \n","    # tokenize articles\n","    tokenized_articles = []\n","    for article in articles:\n","        tokenized_articles.append(tokenizer.process(article))\n","    \n","    return tokenized_articles\n","\n","def preprocess_articles(articles, language):\n","    \"\"\"\n","    articles: list of articles tokenized by stanza\n","    language: \"english\" or \"dutch\"\n","\n","    returns list of articles with list of sentences with list of filtered and preprocessed words\n","    \"\"\"\n","    stop_words = set(stopwords.words(language))\n","    \n","    preprocessed_articles = []\n","    for article in articles:\n","        all_tokens = []\n","        # get lowercase tokens without stopwords or punctuation:\n","        for sentence in article.sentences:\n","            tokens_sent = [token.text.lower() for token in sentence.tokens if not token.text.lower() in stop_words]\n","            all_punctuation = string.punctuation + \"“”—’‘„\"\n","            clean_tokens = [token for token in tokens_sent if not token in all_punctuation]\n","            all_tokens.append(clean_tokens)\n","        preprocessed_articles.append(all_tokens)\n","    \n","    return preprocessed_articles\n","\n","def filter_rare_words(docs, frequency_threshold):\n","    \"\"\"\n","    This functions filters words occuring less than the frequency threshold.\n","    docs: list of articles with list of sentences with list of tokenized words\n","    \"\"\"\n","    alltokens = [token for doc in docs for sentence in doc for token in sentence]\n","\n","    # Count frequency of tokens and determine the infrequent ones:\n","    kw_counter = Counter(alltokens)\n","    infrequent_tokens = Counter({k: c for k, c in kw_counter.items() if c < frequency_threshold})\n","    \n","    # filtering out the infrequent words\n","    frequent_docs = []\n","    for doc in docs:\n","        sentences_2 = []\n","        for sentence in doc:\n","            tokens_2 = []\n","            for token in sentence:\n","                if token not in infrequent_tokens.keys():\n","                    tokens_2.append(token)\n","            sentences_2.append(tokens_2)\n","        frequent_docs.append(sentences_2)\n","    \n","    return frequent_docs\n","\n","\n","### GETTING EMBEDDING FOR WORDS ###\n","def featureVecMethod(list_of_sents, # A list of tokenized sentences\n","                     model, # The actual word embeddings model\n","                     modelword_index, # An index on the vocabulary of the model to speed up lookup\n","                     num_embedding_dimensions # the number of dimensions of the embedding model\n","                    ):\n","    list_of_words = []\n","    for sent in list_of_sents:\n","        list_of_words.extend(sent)\n","\n","    # Pre-initialising empty numpy array (np) for speed\n","    # This create a numpy array with the length of the num_features set to zero values\n","    featureVec = np.zeros(num_embedding_dimensions,dtype=\"float32\")\n","\n","    ## A counter for the number of tokens represented so that we can take the average\n","    nwords = 0\n","    embedding_words = []\n","    no_embeddings_words = []\n","\n","    for word in list_of_words:\n","        if word in modelword_index:\n","            ### Instead of simply taking the embedding values we prefer the next function that\n","            ### normalises these values between [-1, 1] to make the average work better\n","            featureVec = np.add(featureVec,model[word]/np.linalg.norm(model[word]))\n","\n","            # keep track of the words detected, just for analysing the data\n","            embedding_words.append(word)\n","            nwords = nwords + 1\n","        else:\n","            # keep track of the unknown words to see how well our model fits the data\n","            no_embeddings_words.append(word)\n","\n","    # Dividing the result by number of words in each utterance to get average\n","    if nwords>0:\n","        featureVec = np.divide(featureVec, nwords)\n","\n","    return featureVec, embedding_words, no_embeddings_words\n","\n","# Function for calculating the average feature vector\n","def getAvgFeatureVecs(docs, ### List of documents of lists of sentences\n","                      model,\n","                      modelword_index,\n","                      num_embedding_dimensions\n","                     ):\n","    counter = 0\n","    embedding_words=[]\n","    no_embeddings_words=[]\n","\n","    textFeatureVecs = np.zeros((len(docs),num_embedding_dimensions),dtype=\"float32\")\n","    print('Shape of our matrix is:',textFeatureVecs.shape)\n","\n","    #### Iterate over all the texts\n","    for doc in docs:\n","        # Printing a status message every 1000th text, to see what we are processing\n","        if counter%20 == 0:\n","            print(\"Vectorizing %d of %d\"%(counter,len(docs)))\n","            # Checkpoint\n","        ### Each text is transformed into a vector representation based on the averaged token embedding using the previous function\n","        ### Add these vectors to the total matrix\n","        textFeatureVecs[counter], emb_words, nemb_words = featureVecMethod(doc, model, modelword_index,num_embedding_dimensions)\n","        counter = counter+1\n","        embedding_words.append(emb_words)\n","        no_embeddings_words.append(nemb_words)\n","\n","    #### Due to the averaging, there could be infinitive values or NaN values. The next numpy function turns these value to \"0\" scores\n","    textFeatureVecs = np.nan_to_num(textFeatureVecs)\n","\n","    return textFeatureVecs, embedding_words, no_embeddings_words\n","\n","### PLOTTING IN TWO DIMENSIONS ###\n","# Reduce vector dimensions\n","def reduce_dimensions(word_vectors, size_vocab):\n","    model = word_vectors\n","\n","    size_vocab = size_vocab\n","    vocab = [model.index_to_key[i] for i in range(size_vocab)]\n","\n","    high_dimensional = [model[w] for w in vocab]\n","    reduction_technique = TSNE(n_components=2)\n","\n","    print(\"Calculate dimensionality reduction\")\n","    two_dimensional = reduction_technique.fit_transform(high_dimensional)\n","    print(\"Done\")\n","    return two_dimensional\n","\n","# Get the indices for most frequent terms\n","def get_index_for_frequent_terms(known_words, word_vectors, size_vocab):\n","    frequencies = Counter()\n","\n","    for doc in known_words:\n","        frequencies.update(doc)\n","\n","    # find 30 most frequent terms\n","    frequencies_top30 = OrderedDict(frequencies.most_common(30))\n","    terms = list(frequencies_top30.keys())\n","    print(terms)\n","\n","    term_indices = [word_vectors.key_to_index[t] for t in terms]\n","    # print(term_indices)\n","\n","    # remove terms + their indices that are not in the loaded vocabulary (size_vocab = 12000, see previous cell)\n","    remove = []\n","    for i, index in enumerate(term_indices):\n","        if index > size_vocab:\n","            remove.append(i)\n","\n","    length = len(terms)\n","    terms = [terms[i] for i in range(length) if i not in remove]\n","    term_indices = [term_indices[i] for i in range(length) if i not in remove]\n","\n","    return terms, term_indices\n","\n","def plot_embedding_scatter(terms, term_indices, two_dimensional, iso1):\n","    fig, ax = plt.subplots(1, 1, figsize = (15, 10))\n","\n","    # Plot the two-dimensional vectors for the selected terms\n","    x_values = [two_dimensional[index, 0] for index in term_indices] # if index < 12000\n","    y_values = [two_dimensional[index, 1] for index in term_indices]\n","\n","    ax.plot(x_values, y_values, 'o')\n","\n","    if iso1 == \"en\":\n","        color = 'm'\n","        lang = 'English'\n","    if iso1 == \"nl\":\n","        color = 'c'\n","        lang = 'Dutch'\n","\n","    for x, y in zip(x_values, y_values):\n","        ax.plot(x, y, 'o', markersize=12, color=color)\n","\n","    ax.set_title(f'2D word vectors for most frequent {lang} terms', fontsize=20)\n","\n","    ax.set_yticks([])\n","    ax.set_xticks([])\n","\n","    # Annotate the terms in the plot\n","    for i, word in enumerate(terms):\n","        plt.annotate(word, xy=(x_values[i], y_values[i]), fontsize=14)\n","\n","    plt.show()\n","  ###\n","\n","### MAKING THE WORD CLOUDS ###\n","# Use word cloud to evaluate the cluster\n","def wordcloud_cluster_byIds(clusterId, clusters, doc):\n","    words = []\n","    for i in range(0, len(clusters)):\n","        if clusters[i] == clusterId:\n","            for word in doc[i]:\n","                words.append(word)\n","    # Only printing the results I want:\n","    # if len(words)==\n","    print(\"Nr of words in cluster:\", len(words))\n","    print(\"Samples\", words[::max((len(words)//10), 1)])\n","    # Generate a word cloud based on the frequency of the terms in the cluster\n","    wordcloud = WordCloud(max_font_size=40, relative_scaling=.8).generate(' '.join(words))\n","\n","    plt.figure()\n","    plt.imshow(wordcloud)\n","    plt.axis(\"off\")\n","    plt.savefig(str(clusterId)+\".png\")\n","\n","def making_word_clouds(vectors, num_clusters, known_words, random):\n","    km = KMeans(n_clusters=num_clusters, random_state=random)\n","    km.fit(vectors)\n","\n","    clusters = km.labels_.tolist()\n","    for ind in range(0, num_clusters):\n","        wordcloud_cluster_byIds(ind, clusters, known_words)\n","    return clusters\n","\n","def map_cluster_to_source(cluster_list, iso1):\n","    # Mapping the clusters to metadata\n","    if iso1==\"en\":\n","        news_content = news_content_eng\n","    if iso1==\"nl\":\n","        news_content = news_content_nld\n","    clustered_articles ={'Title': news_content[\"Title\"],'Author': news_content[\"Author\"],'Source': news_content[\"Source\"], 'Cluster': cluster_list}\n","    overview = pd.DataFrame(clustered_articles, columns = ['Author', 'Title', 'Source', 'Cluster'])\n","    for num in set(cluster_list):\n","        cluster = overview[overview['Cluster']==num]\n","\n","        # Counting the sources of each clusters\n","        print(f\"Info of cluster {num+1}\")\n","        print(cluster.info())\n","        print(f\"Source count of cluster {num+1}:\")\n","        print(cluster['Source'].value_counts())\n","###\n","\n","### LOADING WORD FEATURES FROM DATABASES ###\n","\n","# function to load mrc database\n","def load_mrc(file_name):\n","    \"\"\"\n","    function that loads the mrc database and extracts features\n","    returns dictionary with Brown frequency, familiarity, concreteness and age of acquisition for each word\n","    \"\"\"\n","    words ={}\n","    print(\"Loading mrc...\")\n","    for line in open(file_name,'r'):\n","        \n","        # This code is from https://github.com/samzhang111/mrc-psycholinguistics/blob/master/extract.py\n","        word, phon, dphon, stress = line[51:].split('|')\n","\n","        nlet = int(line[0:2])\n","        nphon = int(line[2:4])\n","        nsyl = int(line[4])\n","        kf_freq = int(line[5:10])\n","        kf_ncats = int(line[10:12])\n","        kf_nsamp = int(line[12:15])\n","        tl_freq = int(line[15:21])\n","        brown_freq = int(line[21:25])\n","        fam = int(line[25:28])\n","        conc = int(line[28:31])\n","        imag = int(line[31:34])\n","        meanc = int(line[34:37])\n","        meanp = int(line[37:40])\n","        aoa = int(line[40:43])\n","        tq2 = line[43]\n","        wtype = line[44]\n","        pdwtype = line[45]\n","        alphasyl = line[46]\n","        status = line[47]\n","        var = line[48]\n","        cap = line[49]\n","        irreg = line[50]\n","       \n","        words[word.lower()] = (int(brown_freq), int(conc), int(aoa))\n","    print(\"Done.\")\n","    return words\n","\n","# calculating Dutch features: word frequencies, concreteness, age of acquisition\n","def alpino_frequency():\n","    \"\"\"\n","    function that provides a dictionary of all words and their frequency in the Alpino corpus (Dutch)\n","    \"\"\"\n","    print(\"loading alpino frequency...\")\n","    corpus = \" \".join([word.lower() for word in alpino.words()])\n","    tokenized_corpus = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(corpus)]\n","    tokenized_corpus = tokenized_corpus[0]\n","    Alpino_counter = Counter(tokenized_corpus)\n","    print(\"Done.\")\n","    \n","    return Alpino_counter\n","\n","def concreteness_nld(file_name, sheet):\n","    \"\"\"\n","    function that provides a dictionary of Dutch words and their concreteness\n","    ratings from: http://crr.ugent.be/archives/1602\n","    \"\"\"\n","    print(\"loading concreteness nld...\")\n","    df = pd.read_excel(io=file_name, sheet_name=sheet, header=0)\n","    conc_dict = dict([(word, conc) for word, conc in zip(df.stimulus, df.Concrete_m)])\n","    print(\"Done.\")\n","    \n","    return conc_dict\n","\n","def aoa_nld(file_name):\n","    \"\"\"\n","    function that provides a dictionary of Dutch words and their age of acquisition\n","    ratings from: http://crr.ugent.be/archives/1602\n","    \"\"\"\n","    print(\"loading aoa nld...\")\n","    df = pd.read_excel(io=file_name, header=0, skiprows=[1])\n","    aoa_dict = dict([(word, aoa) for word, aoa in zip(df.Word, df.Average)])\n","    print(\"Done.\")\n","    \n","    return aoa_dict\n","\n","\n","### CALCULATE FEATURE VECTORS ###\n","\n","# Functions that calculates average of a feature over tokens for English and Dutch, using the databases defined above\n","def calculate_eng_features(tokens, mrc, n):\n","    \"\"\"\n","    tokens = list of tokens\n","    mrc = dictionary containing several features for each word in mrc database\n","    n = nr of features outputted by mrc (length tuple (value) for each word (key))\n","    \n","    returns list of length n of the average rating over the tokens for each of the n mrc_features\n","    \"\"\"\n","    \n","    feature_lists = [[] for i in range(n)]\n","    stats = []\n","    \n","    for token in tokens:\n","        # For words that are not in the database, we assign the rating 0\n","        feature_ratings = mrc.get(token.lower(), (0,0)) # returns tuple with ratings for all specified features for this token\n","        \n","        # We only consider words that have a rating in the database\n","        for i, rating in enumerate(feature_ratings):\n","            if rating > 0:\n","                feature_lists[i].append(rating)\n","             \n","    # Take the mean over all tokens for every feature and append to stats:\n","    for feature in feature_lists:\n","        if len(feature) > 0:\n","            stats.append(statistics.mean(feature))\n","        else:\n","            stats.append(0.0)\n","    \n","    return stats\n","\n","def calculate_nld_features(tokens, alpino_counter, conc_dict, aoa_dict):\n","    \"\"\"\n","    tokens = list of tokens\n","    alpino_counter = dictionary containing frequency for each word in alpino corpus\n","    conc_dict = dictionary containing concreteness ratings of Dutch words\n","    aoa_dict = dictionary containing age of acquisition ratings of Dutch words\n","    \n","    returns the average of each feature over the tokens\n","    \"\"\"\n","    feature_lists = [[] for i in range(3)]\n","    stats = []\n","    \n","    for token in tokens:\n","        # For words that are not in the database, we assign the rating 0\n","        freq = alpino_counter.get(token.lower(), 0)\n","        conc = conc_dict.get(token.lower(), 0)\n","        aoa = aoa_dict.get(token.lower(), 0)\n","        \n","        # We only consider words that have a rating in the database\n","        for i, rating in enumerate([freq, conc, aoa]):\n","            if rating > 0:\n","                feature_lists[i].append(rating)\n","    \n","    # Take the mean over all tokens for every feature and append to stats:\n","    for feature in feature_lists:\n","        if len(feature) > 0:\n","            stats.append(statistics.mean(feature))\n","        else:\n","            stats.append(0.0)\n","    \n","    return stats\n","\n","# Function that turns articles into feature vectors:\n","def feature_vectorize(docs, language, mrc, alpino_counter, conc_dict, aoa_dict):\n","    \"\"\"\n","    docs: list of articles with list of sentences with list of tokens\n","    language: \"eng\" or \"nld\"\n","    mrc: for language = \"eng\"\n","    alpino_counter, conc_dict, aoa_dict: for language = \"nld\"\n","    \n","    returns vector representation for each article based on several defined features.\n","    \"\"\"\n","\n","    all_docs =[]\n","\n","    for article in docs:\n","        doc_representation = []\n","\n","        # Calculate TTR + average sentence length + average token length\n","        token_frequencies = Counter()\n","        sentence_lengths = []\n","        token_lengths = []\n","        for sentence in article:\n","            tokens_sent = [token for token in sentence]\n","            token_frequencies.update(tokens_sent)\n","            sentence_lengths.append(len(sentence))\n","            token_lengths = token_lengths + [len(token) for token in sentence]\n","        num_types = len(token_frequencies.keys())\n","        num_tokens = sum(token_frequencies.values())\n","        tt_ratio = num_types/float(num_tokens)\n","        doc_representation.append(tt_ratio) # [0] type-token ratio\n","        doc_representation.append(statistics.mean(sentence_lengths)) # [1] avg sentence length\n","        doc_representation.append(statistics.mean(token_lengths)) # [2] avg token length\n","\n","        if language == \"eng\":\n","            # Calculate additional features\n","            tokens = [word for sentence in article for word in sentence]\n","            stats = calculate_eng_features(tokens, mrc, 3)\n","            doc_representation.append(stats[0]) # [3] Brown frequency\n","            doc_representation.append(stats[1]) # [4] concreteness\n","            doc_representation.append(stats[2]) # [5] age of acquisition\n","            \n","        if language == \"nld\":\n","            # Calculate additional features\n","            tokens = [word for sentence in article for word in sentence]\n","            stats = calculate_nld_features(tokens, alpino_counter, conc_dict, aoa_dict)\n","            doc_representation.append(stats[0]) # [3] Alpino frequency\n","            doc_representation.append(stats[1]) # [4] concreteness\n","            doc_representation.append(stats[2]) # [5] age of acquisition\n","\n","        all_docs.append(doc_representation)\n","    \n","    return all_docs\n","\n","### CLUSTERING BASED ON FEATURE VECTORS ###\n","\n","# function that standardizes the feature vectors\n","def standardize_array(array):\n","    \"\"\"\n","    array: instances on rows, features on columns\n","    returns and array with z-scored values based on the mean and std of the columns\n","    \"\"\"\n","\n","    array_standardized = np.zeros(array.shape)\n","    Means = []\n","    SDs = []\n","    \n","    # first, calculate the mean and standard deviation for all features\n","    # we need these for the z-scoring later\n","    n_features =  np.size(array, 1)\n","    for i in range(n_features):\n","        mean = statistics.mean(array[:,i])\n","        Means.append(mean)\n","        sd = statistics.stdev(array[:,i])\n","        SDs.append(sd)\n","\n","    # standardizing (i.e. z-scoring) all values in all_stats and adding them to the new array\n","    for i, row in enumerate(array):\n","        for j, value in enumerate(row):\n","            z_score = (value - Means[j]) / SDs[j]\n","            array_standardized[i, j] = z_score\n","\n","    # array_standardized contains the z-scored features, i.e. M = 0, SD = 1\n","    return array_standardized\n","\n","# function that calculates the average over the features within each cluster\n","def avg_features_clusters(feature_vecs, cluster_list):\n","    \"\"\"\n","    function that calculates the average over the features within each cluster\n","    \n","    feature_vecs: document vectors as numpy array\n","    n_clusters: number of clusters\n","    cluster_list: list of clusters for all articles\n","    \"\"\"\n","    n_clusters = len(set(cluster_list))\n","    avg_features_per_cluster = []\n","    clusters = np.array(cluster_list)\n","    # for each cluster, calculate average over features\n","    for i in range(n_clusters):\n","        # get indices of articles within this cluster\n","        indices_cluster = list(np.where(clusters == i)[0])\n","        # select these articles in feature_vecs and calculate the average over the columns (i.e. features)\n","        feature_vecs_cluster = feature_vecs[indices_cluster]\n","        means = np.mean(feature_vecs_cluster, axis=0)\n","        avg_features_per_cluster.append(list(np.around(means, decimals = 3)))\n","\n","    return avg_features_per_cluster"]},{"cell_type":"code","source":["#############################################\n","### READING IN THE DATA AND PREPROCESSING ###\n","#############################################\n","\n","# Read in the English tsv file as a pandas dataframe\n","filepath = \"data/eng/abortion_overview_clean.tsv\"\n","news_content_eng = pd.read_csv(filepath, sep=\"\\t\", header = 0, keep_default_na=False)[:100]\n","# Tokenize the articles\n","tokenized_articles_eng = tokenize_articles(news_content_eng, \"en\")\n","# Preprocess the articles\n","docs_eng = preprocess_articles(tokenized_articles_eng, \"english\")\n","\n","# Read in the Dutch tsv file as a pandas dataframe\n","filepath = \"data/nld/abortus_overview_clean.tsv\"\n","news_content_nld = pd.read_csv(filepath, sep=\"\\t\", header = 0, keep_default_na=False)[:100]\n","# Tokenize the articles\n","tokenized_articles_nld = tokenize_articles(news_content_nld, \"nl\")\n","# Preprocess the articles\n","docs_nld = preprocess_articles(tokenized_articles_nld, \"dutch\")\n","\n","# Filter infrequent words (noise)\n","frequency_threshold = 2\n","docs_eng_filtered = filter_rare_words(docs_eng, frequency_threshold)\n","docs_nld_filtered = filter_rare_words(docs_nld, frequency_threshold)\n","\n","### BASIC STATS ###\n","\n","# Calculate average number of sentences of the articles\n","nr_sentences_per_article = []\n","for article in docs_eng:\n","    nr_sentences_per_article.append(len(article))\n","print(\"Average nr of sentences per article (English): \", np.mean(nr_sentences_per_article))\n","\n","nr_sentences_per_article = []\n","for article in docs_nld:\n","    nr_sentences_per_article.append(len(article))\n","print(\"Average nr of sentences per article (Dutch): \", np.mean(nr_sentences_per_article))\n","\n","# Calculate average number of words of the articles\n","nr_words_per_article = []\n","for article in docs_eng_filtered:\n","    nr_words = 0\n","    for sentence in article:\n","        nr_words += len(sentence)\n","    nr_words_per_article.append(nr_words)\n","print(\"Average nr of words per article (English): \", np.mean(nr_words_per_article))\n","\n","nr_words_per_article = []\n","for article in docs_nld_filtered:\n","    nr_words = 0\n","    for sentence in article:\n","        nr_words += len(sentence)\n","    nr_words_per_article.append(nr_words)\n","print(\"Average nr of words per article (Dutch): \", np.mean(nr_words_per_article))\n","\n","# Number of different sources + top 3\n","print(\"Nr. of different sources: \", news_content_eng[\"Source\"].nunique())\n","print(\"Top 3 most occuring sources: \\n\", news_content_eng[\"Source\"].value_counts()[:3].sort_values(ascending=False)) # most popular\n","\n","print(\"Nr. of different sources: \", news_content_nld[\"Source\"].nunique())\n","print(\"Top 3 most occuring sources: \\n\", news_content_nld[\"Source\"].value_counts()[:3].sort_values(ascending=False)) # most popular\n","\n","#########################\n","### SEMANTIC ANALYSES ###\n","#########################\n","\n","### ENGLISH ###\n","\n","# Loading the vector model: eng\n","print(\"loading English vector model\")\n","word_vectors_eng = KeyedVectors.load_word2vec_format(\"data/wiki-news-300d-1M.vec\")\n","print(\"done loading\")\n","\n","## Getting the word vectors\n","#Converting Index2Word which is a list to a set for better speed in the execution.\n","#Allows for quicker lookup if the words exist\n","index2key_set_eng = set(word_vectors_eng.index_to_key)\n","num_features = 300\n","# Calculating average vector for Eng set\n","eng_vectors, eng_known_words, eng_unknown_words = getAvgFeatureVecs(docs_eng_filtered, word_vectors_eng, index2key_set_eng, num_features)\n","\n","# Plotting the English frequent words in two dimensions\n","size_vocab = 12000\n","two_dimensional_eng = reduce_dimensions(word_vectors_eng, size_vocab)\n","terms_eng, term_indices_eng = get_index_for_frequent_terms(eng_known_words, word_vectors_eng, size_vocab)\n","plot_embedding_scatter(terms_eng, term_indices_eng, two_dimensional_eng, \"en\")\n","\n","## Clustering\n","# We will try 2, 3 and 4 clusters to see the difference\n","# Each number of clusters will be performed 3 times\n","\n","# Defining a random seed\n","random.seed(0)\n","\n","# Testing k number 2, 3 or 4, each run 3 times\n","for k in [2, 3, 4]:\n","    for i in range(3):\n","        state = random.randint(0, 42)\n","        # Cluster\n","        # clusters = making_word_clouds(eng_vectors, k, eng_known_words, state)\n","        # map_cluster_to_source(clusters, 'en')\n","\n","        # After observation, only printing the results in the analysis\n","        if k==4 and i==1:\n","            clusters = making_word_clouds(eng_vectors, k, eng_known_words, state)\n","            map_cluster_to_source(clusters, 'en')\n","\n","### DUTCH ###\n","\n","# Loading the vector model: nld\n","print(\"loading Dutch vector model\")\n","word_vectors_nld = KeyedVectors.load_word2vec_format(\"data/cc.nl.300.vec\")\n","print(\"done loading\")\n","\n","## Getting the word vectors\n","# Converting Index2Word which is a list to a set for better speed in the execution.\n","# Allows for quicker lookup if the words exist\n","index2key_set_nld = set(word_vectors_nld.index_to_key)\n","num_features = 300\n","# Calculating average vector for training set\n","nld_vectors, nld_known_words, nld_unknown_words = getAvgFeatureVecs(docs_nld_filtered, word_vectors_nld, index2key_set_nld, num_features)\n","\n","# Plotting the frequent Dutch terms in 2D\n","size_vocab = 12000\n","two_dimensional_nld = reduce_dimensions(word_vectors_nld, size_vocab)\n","terms_nld, term_indices_nld = get_index_for_frequent_terms(nld_known_words, word_vectors_nld, size_vocab)\n","plot_embedding_scatter(terms_nld, term_indices_nld, two_dimensional_nld, \"nl\")\n","\n","## Clustering\n","# Defining a random seed\n","random.seed(42)\n","\n","# Testing k number 2, 3 or 4, each run 3 times\n","for k in [2, 3, 4]:\n","    for i in range(3):\n","        state = random.randint(0, 42)\n","        # Cluster\n","        # clusters = making_word_clouds(nld_vectors, k, nld_known_words, state)\n","        # map_cluster_to_source(clusters, \"nl\")\n","\n","        # After observation, only printing the results in the analysis\n","        if k==3 and i==0:\n","            clusters = making_word_clouds(nld_vectors, k, nld_known_words, state)\n","            map_cluster_to_source(clusters, \"nl\")\n","\n","## CROSSLINGUAL BERT ##\n","\n","# Initialize BERT\n","model_name = 'bert-base-multilingual-cased'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModel.from_pretrained(model_name)\n","\n","# Concatenating the documents\n","all_sents = []\n","\n","for each in (news_content_eng, news_content_nld):\n","    for doc in list(each[\"Text\"]):\n","        sent = doc.split(\".\")\n","        for s in sent:\n","            s = s + \".\"\n","            all_sents.append(s)\n","\n","# Vectorization\n","vectors = []\n","all_tokens = []\n","print(\"Processing...\")\n","for sent in all_sents:\n","    tokens = tokenizer.encode(sent, add_special_tokens=True)\n","    sent = [tokenizer.decode([token]) for token in tokens]\n","    all_tokens.append(sent)\n","    model.eval()  # turn off dropout layers\n","    tokens_tensor = torch.tensor(tokens).unsqueeze(0)\n","    output = model(tokens_tensor)\n","    vector = output[0].detach().numpy()[0]\n","    vectors.append(vector)\n","    if len(vectors)%1000==0:\n","        print(f\"Processing...{len(vectors)}/{len(all_sents)}\")\n","        print(\"Checkpoint\")\n","        print(sent)\n","        print(vector.shape)\n","print(\"Done.\")\n","\n","high_dimensional_tok = []\n","high_dimensional_vec = []\n","# Randomly choose tokens to reduce dimension: use all the tokens in every 3 sentences if it is longer than 20 words\n","for sent in all_tokens[::3]:\n","    if len(sent)>20:\n","        high_dimensional_tok.extend(sent[1:-1])\n","        high_dimensional_vec.extend(vectors[all_tokens.index(sent)][1:-1])\n","\n","reduction_technique = TSNE(n_components=2)\n","\n","print(\"Calculate dimensionality reduction\")\n","two_dimensional = reduction_technique.fit_transform(high_dimensional_vec)\n","print(\"Done\")\n","\n","terms = []\n","term_indices = []\n","for terms_list in (terms_eng, terms_nld):\n","    for term in terms_list:\n","        # If the frequent term is in the reduced dimension:\n","        if term in high_dimensional_tok:\n","            terms.append(term)\n","            term_indices.append(high_dimensional_tok.index(term))\n","\n","fig, ax = plt.subplots(1, 1, figsize = (15, 10))\n","\n","#term_indices = [high_dimensional.index[vec] for vec in term_vec]\n","\n","# Plot the two-dimensional vectors for the selected terms\n","x_values = [two_dimensional[i, 0] for i in term_indices]\n","y_values = [two_dimensional[i, 1] for i in term_indices]\n","\n","ax.plot(x_values, y_values, 'o')\n","\n","for x, y in list(zip(x_values, y_values))[:20]:\n","    ax.plot(x, y, 'o', markersize=12, color='m')\n","for x, y in list(zip(x_values, y_values))[20:]:\n","    ax.plot(x, y, 'o', markersize=12, color='c')\n","\n","ax.set_title(f'2D word vectors for most frequent English and Dutch terms', fontsize=20)\n","\n","# Hide the ticks\n","ax.set_yticks([])\n","ax.set_xticks([])\n","\n","# Annotate the terms in the plot\n","for i, word in enumerate(terms):\n","    plt.annotate(word, xy=(x_values[i], y_values[i]), fontsize=14)\n","\n","plt.show()\n","\n","##########################\n","### STYLISTIC ANALYSES ###\n","##########################\n","\n","random.seed(10)\n","\n","# Here we load the English features from the mrc database: Brown frequency, concreteness and age of acquisition (aoa)\n","file_name = 'data/mrc2.dct.txt'\n","mrc = load_mrc(file_name)\n","\n","# Here, we load the Dutch features: Alpino frequency, concreteness and age of acquisition (aoa)\n","alpino_counter = alpino_frequency()\n","\n","file_name =  'data/concreteness.xlsx'\n","sheet = 'FULLIST'\n","conc_dict = concreteness_nld(file_name, sheet)\n","\n","file_name =  'data/AoA.xlsx'\n","aoa_dict = aoa_nld(file_name)\n","\n","### ENGLISH ARTICLES ###\n","\n","# We turn the English articles into feature vectors\n","# Each article is represented as a vector with 6 dimensions:\n","# Type-token ratio, avg sentence length, avg token length, avg frequency, avg concreteness and avg age of acquisition\n","doc_vectors_eng = feature_vectorize(docs_eng_filtered, \"eng\", mrc, alpino_counter, conc_dict, aoa_dict)\n","# print(doc_vectors_eng[:3])\n","\n","# Calculate average of the features over all articles\n","print(\"Feature vector consists of: [TTR, avg sentence len, avg token len, freq, conc, AoA]\")\n","print(\"Average feature vector over all English articles: \\n\", np.mean(doc_vectors_eng, axis=0))\n","\n","# For k-means, it is important to standardize all features, so that they all contribute equally to the clustering.\n","doc_vectors_eng_stand = standardize_array(np.array(doc_vectors_eng))\n","\n","# We cluster the articles into clusters with k = 2, k = 3 and k = 4\n","# And we repeat this 5 times to check how consistent the results are\n","for k in [2,3,4]:\n","    for i in range(5):\n","        # Cluster\n","        km = KMeans(n_clusters = k)\n","        km.fit(doc_vectors_eng_stand)\n","        \n","        # print how many articles per cluster\n","        clusters_eng = km.labels_.tolist()\n","        counts = []\n","        for j in range(k):\n","            count = clusters_eng.count(j)\n","            counts.append(count)\n","            # If we have a cluster with just one article, we print the outlier\n","            if count == 1:\n","                print(f\"article {clusters_eng.index(j)} is an outlier\")\n","#         print(counts)\n","        \n","        # Calculate the average feature vectors per cluster\n","#         print(avg_features_clusters(doc_vectors_eng_stand, k, clusters_eng))\n","\n","# We choose k = 2 for further analyses, because we want to divide the articles up into \"difficult\" and \"easy\"\n","\n","# Now we cluster the articles into 2 clusters\n","# Keep in mind that Python has 0 indexing, so cluster 0 will correspond to cluster 1 in our paper.\n","k = 2\n","km = KMeans(n_clusters = k)\n","km.fit(doc_vectors_eng_stand)\n","\n","# Add the clusters as a column in our dataframe\n","clusters_eng = km.labels_.tolist()\n","clustered_articles ={'Title': news_content_eng[\"Title\"],'Author': news_content_eng[\"Author\"],'Source': news_content_eng[\"Source\"], 'Cluster': clusters_eng}\n","overview_eng = pd.DataFrame(clustered_articles, columns = ['Title', 'Author', 'Source', 'Cluster'])\n","# overview_eng.sort_values(['Cluster', 'Source'])\n","\n","# count articles\n","print(\"Cluster counts:\")\n","print(overview_eng[\"Cluster\"].value_counts())\n","\n","# Count articles per source per cluster\n","print(overview_eng.groupby(\"Source\")[\"Cluster\"].value_counts())\n","\n","# We calculate the average of each feature within each cluster\n","# We use the standardized feature vectors because then we can compare the differences\n","print(\"Average feature vectors within each cluster (English)\")\n","print(\"[TTR, avg sent length, avg token length, freq, conc, AoA]\")\n","print(avg_features_clusters(doc_vectors_eng_stand, clusters_eng))\n","\n","### DUTCH ARTICLES ###\n","\n","# We turn the Dutch articles into feature vectors\n","# Each article is represented as a vector with 6 dimensions:\n","# Type-token ratio, avg sentence length, avg token length, avg frequency, avg concreteness and avg age of acquisition\n","doc_vectors_nld = feature_vectorize(docs_nld_filtered, \"nld\", mrc, alpino_counter, conc_dict, aoa_dict)\n","# print(doc_vectors_nld[:3])\n","\n","# Calculate average of the features over all articles\n","print(\"Feature vector consists of: [TTR, avg sentence len, avg token len, freq, conc, AoA]\")\n","print(\"Average feature vector over all Dutch articles: \\n\", np.mean(doc_vectors_nld, axis=0))\n","\n","# For k-means, it is important to standardize all features, so that they all contribute equally to the clustering.\n","doc_vectors_nld_stand = standardize_array(np.array(doc_vectors_nld))\n","\n","# We cluster the articles into clusters with k = 2, k = 3 and k = 4\n","# And we repeat this 5 times to check how consistent the results are\n","for k in [2,3,4]:\n","    for i in range(5):\n","        # Cluster\n","        km = KMeans(n_clusters = k)\n","        km.fit(doc_vectors_nld_stand)\n","        \n","        # print how many articles per cluster\n","        clusters_nld = km.labels_.tolist()\n","        counts = []\n","        for j in range(k):\n","            count = clusters_nld.count(j)\n","            counts.append(count)\n","            # If we have a cluster with just one article, we print the outlier\n","            if count == 1:\n","                print(f\"article {clusters_nld.index(j)} is an outlier\")\n","        # print(counts)\n","        \n","        # Calculate the average feature vectors per cluster\n","        # print(avg_features_clusters(doc_vectors_nld_stand, clusters_nld))\n","\n","# We choose k = 2 for further analyses, because we want to divide the articles up into \"difficult\" and \"easy\"\n","\n","# Now we cluster the articles into 2 clusters\n","# Keep in mind that Python has 0 indexing, so cluster 0 will correspond to cluster 1 in our paper.\n","k = 2\n","km = KMeans(n_clusters = k)\n","km.fit(doc_vectors_nld_stand)\n","\n","# Add the clusters as a column in our dataframe\n","clusters_nld = km.labels_.tolist()\n","clustered_articles ={'Title': news_content_nld[\"Title\"],'Author': news_content_nld[\"Author\"],'Source': news_content_nld[\"Source\"], 'Cluster': clusters_nld}\n","overview_nld = pd.DataFrame(clustered_articles, columns = ['Author', 'Title', 'Source', 'Cluster'])\n","# overview_nld.sort_values(['Cluster', 'Source'])\n","\n","# count articles\n","print(\"Cluster counts:\")\n","print(overview_nld[\"Cluster\"].value_counts())\n","\n","# Count articles per source per cluster\n","print(overview_nld.groupby(\"Source\")[\"Cluster\"].value_counts())\n","\n","# We calculate the average of each feature within each cluster\n","# We use the standardized feature vectors because then we can compare the differences\n","print(\"Average feature vectors within each cluster (Dutch)\")\n","print(\"[TTR, avg sent length, avg token length, freq, conc, aoa]\")\n","print(avg_features_clusters(doc_vectors_nld_stand, clusters_nld))\n","\n","\n","### CALCULATE THE TIME IT TOOK TO RUN ALL CODE ###\n","stop = time.perf_counter()\n","timer = stop - start\n","minutes = int(timer / 60)\n","print(f\"Time it took to run the code: {minutes} minutes\")\n","\n"],"metadata":{"id":"JsVsrxnF08c4"},"execution_count":null,"outputs":[]}]}